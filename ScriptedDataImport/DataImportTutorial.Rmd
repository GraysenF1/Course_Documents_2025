---
title: "Data Import Tutorial"
author: "Douglas A. Campbell"
date: "`r format(Sys.Date())`"
output:
  html_document:
    df_print: paged
    keep_md: true
---
## Introduction
Data is courtesy of Nerissa Fisher, UTS Australia

This tutorial introduces import of single and multiple data files into R, following principles of R for Data Science
  https://r4ds.had.co.nz/

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook, residing within a directory with a .Rproj file, and a .git, connected to a GitHub repository, https://github.com/MtADATA3101/ScriptedDataImport.

After significant edits we 'Commit' with a message, then 'Push' the edited version to a GitHub repository.

Follow instructions in
https://happygitwithr.com/

to generate a local cloned repository.

When you execute code chunks by clicking a green arrow, or an option from the 'Run' pull down menu, the results appear beneath the code chunk.

Text outside chunks is not run in R.
Add new chunks by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

Libraries (or 'packages') contain additions to base R.
```{r load libraries}
library(tidyverse)
  #tidyverse set of packages for data wrangling and plotting
library(lubridate)
  #assists with date formats
```

Importing data involves reading data in from a source folder(s), and saving formatted data.
In a simple case the data files are stored in a folder within the .Rproj folder.

In a simple case the data files are .csv, with a single row of column names that will be read as variable names, and one type of data in each column below the first row of names.

As your import skills grow you will cope with more complex, less tidied data.

Other approaches are:
-loading in data from a GoogleSheet;
-accessing a folder outside the project using a file path;
-merging data or metadata from multiple files;
-webscraping

We use an interactive .Rmd to step through the import process; the code within the chunks could be consolidated to a single .R script for running repeatedly.

## Project Variables
Assign project-specific values to the variables 'Project', 'DataIn', 'PlotsPath', and 'ProcessData'.
Then we can use these variables in subsequent code chunks to construct file paths.
These folders must exist in the project directory.
```{r set project path variables}
Project <- "DataTidyTutorial"
DataIn <- "DataIn"
PlotsPath <- "Plots"
ProcessData <- "ProcessData"
```

```{r set file variables}
#data files use different 'encodings' to use binary to represent characters; UTF-8 is the current emerging standard, but some instruments save files using different, obscure encodings.
#http://www.steves-internet-guide.com/guide-data-character-encoding/
#If you import a file and get unexpected characters appearing, you may have imported using the wrong encoding assumptions.
#Encoding is not the same as the file extension type, ex. .csv or .txt

#a tidyverse function
#readr::guess_encoding(file = "yourmysteryfile")
#will guess the encoding for a given file, which is not always obvious upon inspection of the raw data
guess_encoding(read_file_raw(readr_example("mtcars.csv")))

FileEncode <- "UTF-8" 

#data fields in files are separated by a symbol, example, "," for ".csv".
# ".tsv" with a "tab" is a common format in Europe.
Delimiter <- ","

#Data files often have non-data 'header' rows above any data labels or values.
#Setting a fixed number for 'header' rows is a brittle solution. It is better to figure out how to read all data starting at line that contains a key character string
HeaderRows <- 0

#Data files sometimes contain non-data comments, prefixed with a set character
Comment <- '#'
```

## Interactively import a single file
Click the 'Import Dataset' icon in the 'Environment Pane' (top right)
Chose the 'From Text (readr)' option
Browse to /DataIn/190902Linco_R.csv
Click 'Import' to generate a data object automatically named the same as the source file.
The 'Code Preview' window shows you the code that will be run to generate the import.
```{r Import Dataset Code Preview}
library(readr)
X190902Linco_R <- read_csv("DataIn/190902Linco_R.csv")
View(X190902Linco_R)
```

The 'Import Dataset' window allows you to change import options
For example, I can interactively set the name of the imported object
```{r  Import Dataset Code Preview}
library(readr)
ImportExample <- read_csv("DataIn/190902Linco_R.csv")
View(ImportExample)
```

This interactive import is suitable for single file imports, and can help to generate example code for import.

## Show Files in a Folder
Show all files within the 'DataIn' path.
Then generate a character vector containing only the names of data files that meet a set criteria, ".csv"
```{r vector of data files}
list.files(path = file.path(DataIn, fsep = .Platform$file.sep), full.names = TRUE)

DataFiles <- list.files(path = file.path(DataIn, fsep = .Platform$file.sep), pattern = ".csv", full.names = TRUE)

```

```{r data files}
DataFiles
```

Note that 'DataFiles' omits any files in the DataIn that do not meet criteria. Note that any files within the sub-folder 'BadData' are omitted; this is a quick way to manually segregate files from import, without deleting them.

## Read in an example data file, and examine the result
```{r read in an example data file}
DataTest <- read_delim(file = "DataIn/190906Linco_R.csv", delim = Delimiter, comment = Comment, skip = HeaderRows)

DataTest
```
Some column names (variable names) that were generated by the original instrument use non-standard characters, that might cause problems later, ex. "1-qP" might be interpreted as 1 minus a non-existent variable "qP".
We can either rename such variables, or surround them with ticks `1-qP` if using them (note these are `tick` not 'quote' marks).

Note also the display of the data class in the column, just below the variable name.
All elements of a column within a data frame must be of the same data class; this can cause issues combining data.

## Multiple Files
Rather reading in files one at a time, we can read in all the files in the source folder that meet our criteria.
First we generate a function 'read_delim_plus' that extends the capacity of 'read_delim' by adding the source filename as the value of a variable 'Flnm', and adds the date on which the file was originally created. Note that this CDateTime is not a reliable guide to the original data capture date because recopying the file may change this date.
```{r read_delim_plus, warning=FALSE, message=FALSE}
#generate read function using tidyverse read_delim
#read_delim_plus adds filename and cdate
read_delim_plus <- function(Flnm, Delimiter, HeaderRows, Comment){read_delim(file = Flnm, delim = Delimiter, comment = Comment, skip = HeaderRows) %>%
    mutate(Filename = Flnm, CDateTime = ymd_hms(file.info(Flnm)$ctime))
}
```

We use our character vector 'DataFiles' to pass to the purrr::map function that applies our read_delim_plus to each file in turn, and creates a list containing a dataframe from each imported file.
We then expand the list object into a single long dataframe.
There are alternate ways to do this.
We will review purrr::map at a later date.

```{r read data}
#shorter way...
# Data_df <- DataFiles %>% 
#   map_df(~read_delim_plus(Flnm = ., Delimiter = Delimiter, Comment = Comment, HeaderRows = HeaderRows))

#explicit way...
Data <- DataFiles %>% 
  map(~read_delim_plus(Flnm = ., Delimiter = Delimiter, Comment = Comment, HeaderRows = HeaderRows)) %>%
  enframe() %>%
  unnest(cols = value)

# Import fails because in underlying files column 'Treatment_min' is sometimes imported as 'dbl' numeric and sometimes as 'chr' character.

#solve problem with Data column 'Treatment_min' that converted to 'chr', needs to be 'dbl'.
# https://stackoverflow.com/questions/57642316/how-do-i-use-purrrmap-with-dataframe-list-to-modify-column-values-in-specific
Data2 <- DataFiles %>% 
  map(~read_delim_plus(Flnm = ., Delimiter = Delimiter, Comment = Comment, HeaderRows = HeaderRows)) %>%
  map(.$value, .f = ~ .x %>% mutate_at(vars(Treatment_min), ~as.numeric(.))) %>%
  enframe() %>%
  unnest(cols = value)

```

## Inspect the imported data  
It is huge and will cause problems with knitting on RStudio.cloud if we display the entire 'Data'
```{r inspect Data}
#Data
Data2[1:10, 1:10]
```

## Save the imported data for further analyses
.csv is a generic comma separated values format.
.Rds is an internal R data format for rapid re-import into other RNotebooks or scripts.
```{r save Data}
saveRDS(Data2, file = file.path(ProcessData,paste(Project, "Data", ".Rds",sep = ""),fsep = .Platform$file.sep))

write_csv(Data2, file = file.path(ProcessData,paste(Project, "Data",".csv",sep = ""),fsep = .Platform$file.sep))
```

Look at the 'ProcessData' folder and see the names of the saved data, constructed using the ProjectName and 'Data'.

If we change the value for 'ProjectName' in 'Chunk 2 r set project variables' the output will have a new name.

## Common Issues with Imports
1. Data files do not all have the same variables in the same column positions.
This prevents bulk import of multiple files because column labels and contents will not match.

2. Non-standard data column labels (variable names)
Variable names in R should contain only letters, numerals (after the first position) or "_".
  Later data tidying can rename non-standard variables:
  CampbellLabConvention_units
  
3. Multiple data types in the same column
read_delim() does not force data conversions, so all data in a column defaults to the most inclusive data type to accommodate all rows.

As your skills grow your ability to cope with non-tidy data increases.
Google knows everything; including 'tidyverse' in your search query will bias results towards tidyverse solutions.