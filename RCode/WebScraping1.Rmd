title: "Web Scraping Part 1"
author: "Elizabeth Stregger"
output: html_document
---
**General reminders for live coding**

%>% (pipe operator) is Ctrl + Shift + M (Windows) Cmd + Shift + M (mac)
<- assignment operator) is Alt + - (Windows) or Option + - (Mac).
run a line of code from the source editor use Ctrl+Enter (Windows) or Cmd+Enter (Mac)
tab to autocomplete long variable names

Before we get into webscraping, I will introduce the dplyr package. This is the main package you use to transform a data frame. Some of the things you can do include filtering, arranging / sorting

library(tidyverse)
library(nycflights13)

Basic structure of dplyr functions

- first argument is a data frame
- subsewuent argument describes which columns to operate on
- output is always a new data frame

each function is designed to do a single thing well, so we need to use the piple to combine functions to solve a complex function

Dplyr functions operating on rows (using flight data)

filter() keep rows based on values of the columns

```{r filtering}

?filter

flights |> 
  filter(dep_delay > 120)

# combine multiple filters using & or | for OR
flights |> 
  filter(month == 10 & day == 18)
```

arrange
```{r arrange}
flights |> 
  arrange(year, month, day, dep_time)

# descending order

flights_descending <- flights |> 
  arrange(desc(dep_delay))
```

count the number of occurrences of a combination of values

```{r}
flights |> 
  count(origin, dest, sort = TRUE)
```
working with columns

mutate creates new columns from existing columns
select changes which columns are present
rename changes the name of a column
relocate changes the position of a column

```{r mutate}
flights |> 
  mutate( 
    gain = dep_delay - arr_delay,
    speed = distance / air_time * 60
    )
```

select columns based on names of variables

```{r select}
flights |> 
  select (year, month, day)

flights |> select(year:day)
flights |> select(!year:day) #select everything except!
flights |> select(where(is.character))
```

working with groups
group_by and summarize
slice_functions allow you to extract specific rows within each group

```{r}
flights |> 
  group_by(month) |> 
  summarize(delay = mean(dep_delay, na.rm = TRUE))

# n() returns the number of rows in each group - this is very useful
flights |> 
  group_by(month) |> 
  summarize(delay = mean(dep_delay, na.rm = TRUE), 
            n = n())

flights |> 
  group_by(dest) |> 
  slice_max(arr_delay, n = 1) |> 
  relocate(dest)
```


```{r install packages and libraries}
#Install packages once
#install.packages("tidyverse")
install.packages("dplyr")
install.packages("rvest")
install.packages("polite")
install.packages("tidytext")

library("tidyverse")
library("dplyr")
library("rvest")
library("polite")
library("tidytext")
```

Package: dplyr 

Package: polite
Developed by Dmytro Perepolkin see: https://dmi3kno.github.io/polite/

Two main functions “bow” and “scrape” bow introduces the client to the host and asks for permission to scrape (it checks the robots.txt file)
scrape is the main function for retrieving the data

Three pillars of a polite session: seeking permission, taking slowly, and never asking twice

**Example one: Instagram**
Use polite to bow to a website that does not allow scraping.
```{r Instagram session}
instagram_session <- bow("https://www.instagram.com/")
instagram_session
```

Even if the path is scrapable, check Terms of Use for websites to see if scraping is allowed. If there is an API (application programming interface) use it instead.

If you’re confident with CSS and HTML, can use source code or developer view to identify the parts of the website that you want to scrape.

Handy tool: SelectorGadget 
SelectorGadget is an interactive JavaScript bookmarklet that helps you figure out CSS selectors for extracting components from a page.
Let’s install SelectorGadget - works in Chrome 
[https://rvest.tidyverse.org/articles/selectorgadget.html]

**Example two: StatCan Daily**

```{r bow to StatCan}
statcan_session <- bow("https://www150.statcan.gc.ca/n1/dai-quo/index-eng.htm?HPA=1")
statcan_session
```
Check terms and conditions on StatCan site: [https://www.statcan.gc.ca/en/reference/terms-conditions/general]
No mention of scraping or robots

```{r scrape StatCan daily headlines}
statcan_titles <- scrape(statcan_session) |> 
  html_elements("h3")
statcan_titles
```

```{r extract titles}
stat_can_titles_extract <- statcan_titles %>%
  html_text2()
stat_can_titles_extract
```

**Example three: Project Gutenberg**

```{r bow to Project Gutenberg}
gutenberg_session <- bow("https://www.gutenberg.org/")
gutenberg_session
```
Check Terms of Use: [https://www.gutenberg.org/policy/terms_of_use.html]
Note: “Any perceived use of automated tools to access this website will result in a temporary or permanent block of your IP address.”

However, there is an R package called gutenbergr that we can use. [https://github.com/ropensci/gutenbergr]

```{r install devtools and gutenbergr}
install.packages("devtools")
library("devtools")
devtools::install_github("ropensci/gutenbergr")
library("gutenbergr")
```

To get to know the gutenbergr functions, look at github documentation or use help
```{r find Dracula}
gutenberg_works(title == "Dracula")
#note the gutenberg_id: 345

dracula <- gutenberg_download(345)
dracula
```

Tidy Text in R
Now that we’ve downloaded some textual data, I’ll introduce ideas for working with tidy text.

My example is adapted from: Text Mining with R: A Tidy Approach
Julia Silge and David Robinson [https://www.tidytextmining.com/index.html]

Tidy data (Hadley Wickham):
- Each variable is a column
- Each observation is a row
- Each type of observational unit is a table

Tidy text: A table with one token per row. 
A token is a meaningful unit of text. 
Tokenization is the process of splitting text into tokens.
If the text is tidy, can use other R functions we’ve introduced in the course.

Another approach: creating a raw string, or “corpus”

```{r Tidy Dracula}
#unnesting the tokens makes a new data frame with one word per row. It also removes capitalization.

dracula_unnest <- dracula |> 
  unnest_tokens(word, text)
dracula_unnest

# remove stop words, extremely common words such as "the"
# tidytext has a dataset called stop_words

data("stop_words")
stop_words

dracula_unnest_no_stopwords <- dracula_unnest |> 
  anti_join(stop_words)

# use count function from dplyr to find most common words

dracula_unnest_no_stopwords %>%
  count(word, sort = TRUE)

dracula_unnest_no_stopwords %>%
  count(word, sort = TRUE) %>%
  filter(n > 175) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col()
```

Sentiments

Tidytext includes three general purpose sentiment lexicons:
AFINN: score from -5 (negative) to 5 (positive) sentiment
bing: positive or negative
nrc: categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust

Note there are significant limitations due to lack of context.
```{r sentiments}
install.packages("textdata")
library("textdata")

nrc_sentiments <- get_sentiments("nrc")

# Note that you have to agree to cite the authors of nrc

nrc_joy <- nrc_sentiments |> 
  filter(sentiment == "joy")

nrc_dracula <- left_join(x = dracula_unnest_no_stopwords, y = nrc_sentiments, by = "word")

nrc_dracula_count <- nrc_dracula |>
  count(word, sentiment, sort = TRUE)

nrc_dracula_count %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(n, word, fill=sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y")

# custom_stop_words <- bind_rows(tibble(word = c("van"), lexicon = c("custom")), stop_words)
```