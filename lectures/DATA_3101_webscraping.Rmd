---
title: "Web scraping with rvest"
output: github_document
---

Webscraping in R is done with a library called [rvest](https://rvest.tidyverse.org/) (like harvest, hahaha, good pun). It is inspired by Python libraries like Beautiful Soup. It is included in the tidyverse but the library still needs to be loaded. The function in rvest help us work with HTML.

We can also use [polite](https://dmi3kno.github.io/polite/) to seek permission to scrape, take slowly, and never ask twice. Polite includes two functions: bow and scrape.

We're going to dip into [Chapters 15 Regular expressions](https://r4ds.hadley.nz/regexps) and [Chapter 26 Iteration](https://r4ds.hadley.nz/iteration) so that we can use *Theory and Applications of Categories (TAC)* as our example. An additional resource I used is [Web Scraping using R.](https://jakobtures.github.io/web-scraping/index.html)

## Getting Started

```{r load libraries}
library(tidyverse) 

library(polite) 

library(rvest) 
```

Task 1: Check if *Theory and Applications of Categories (TAC)* index page can be scraped.

```{r bow to TAC index}
bow("http://www.tac.mta.ca/tac/") 
```

Task 2: Check a site like Instagram.

```{r bow to Instagram}
bow("https://www.instagram.com/") 
```

Task 3: Scrape one article using read_html() function from rvest

```{r scrape one article}
article_42_1 <- read_html("http://www.tac.mta.ca/tac/volumes/42/1/42-01abs.html") 
```

## Choosing What to Scrape

### Function: html_elements()

```{r experiment with finding selectors that match data we need}
article_42_1 |> html_elements("h2") 
```

What we found so far:

-   html_elements("h1") selects the article title

-   html_elements("h2") selects the authors

-   html_elements("p") selects multiple paragraphs

### Function: html_element()

html_element() retrieves the first match.

Test with p

```{r extract first p}
article_42_1 |> html_element("p") 
```

### Function: html_text2()

html_text2() retrieves the text only.

```{r extract text}

article_42_1 |> html_element("h1") |> 
  html_text2() 
```

### Function: html_attr()

html_attr() retrieves the value in an attribute. An example of an element with an attribute is the tag "a" and the attribute "href"

```{r extract a href}
article_42_1 |> html_elements("a") |>  
  html_attr("href") 
```

Combine some of these fields to create a tibble:

```{r authors, articleTitle, articleAbstract}
tibble( 
  authors = article_42_1 |>  
    html_element("h2") |>  
    html_text2(), 
  articleTitle = article_42_1 |>  
    html_element("h1") |>  
    html_text2(), 
  articleAbstract = article_42_1 |>  
    html_element("p") |>  
    html_text2() 
) 
```

## CEWIL Project

Ok, so now we'll get into the problems we'd have to solve to get this into an XML file that we could import into OJS. Luckily we can build off existing work done by [University of Alberta Libraries](https://github.com/ualbertalib/ojsxml).

In their workflow, they transformed a CSV file into XML. We can use webscraping in R to create a CSV file for each volume and then use their application to generate the XML file we need for Open Journal Systems.

The CSV must be in the format of: issueTitle,sectionTitle,sectionAbbrev,authors,affiliation,DOI,articleTitle,year,datePublished,volume,issue,startPage,endPage,articleAbstract,galleyLabel,authorEmail,fileName,keywords,citations,cover_image_filename,cover_image_alt_text,licenseUrl,copyrightHolder,copyrightYear,locale_2,issueTitle_2,sectionTitle_2,articleTitle_2,articleAbstract_2 

### Problems to solve

Extracting values from strings like: Vol. 42, 2024, No. 1, pp 2-8.

How? [Chapter 15: Regular expressions](https://r4ds.hadley.nz/regexps)

Scraping the HTML for each article and then moving on to the next one.

How? Programming! [Chapter 26: Iteration](https://r4ds.hadley.nz/iteration)

### Scraping of multi-page websites

We can use the *TAC* homepage to identify the links we want to scrape.

```{r scrape index}
tac_index_html <- "http://www.tac.mta.ca/tac/" |>  
  read_html() 
```

Task: Identify the links to the abstract pages.

```{r make a list of links}
tac_links <- tac_index_html |>  
  html_elements("a") |>  
  html_attr("href") |>  
  str_subset("41-..abs") |>  
  str_unique() 
```

```{r add prefix to links}
tac_links <- str_glue("http://www.tac.mta.ca/tac/{tac_links}") 
```

Scrape all pages for vol 41:

I referred to [Web Scraping using R by Jakob Tures](https://jakobtures.github.io/web-scraping/rvest3.html) to find out how to read multiple pages. This is accomplished using the map() function, which is covered in the Iteration chapter of R4DS.

The map function applies a function to each element of a vector.

We have a list of links saved as a character vector. In this case the function we want to use is read_html.

```{r read each page}
pages <- tac_links |>  
  map(read_html) 
```

The output, pages, is a list. Each item in the list includes the HTML for one article.

We can now use map for each of the HTML elements we used as selectors earlier.

```{r use map to find title or another column variable}
pages |>  
  map(html_elements, "h1") |>  
  map_chr(html_text2) 
```

And now we can start building a tibble, working towards the specifications for the University of Alberta CSV file:

issueTitle,sectionTitle,sectionAbbrev,authors,affiliation,DOI,articleTitle,year,datePublished,volume,issue,startPage,endPage,articleAbstract,galleyLabel,authorEmail,fileName,keywords,citations,cover_image_filename,cover_image_alt_text,licenseUrl,copyrightHolder,copyrightYear,locale_2,issueTitle_2,sectionTitle_2,articleTitle_2,articleAbstract_2 

```{r create tibble: authors, articleTitle, articleAbstrat}
tibble( 
  authors = pages |>  
    map(html_elements, "h2") |>  
    map_chr(html_text2), 
  articleTitle = pages |>  
    map(html_element, "h1") |>  
    map_chr(html_text2), 
  articleAbstract = pages |>  
    map(html_element, "p") |>  
    map_chr(html_text2) 
) 
```

### Class 2

We've succeeded in scraping the 53 abstract pages for vol. 41 of *TAC.* So far we've identified that h2 can be used for authors, h1 can be used for title, and that the first p is the abstract.

Here are the other pieces of information we need to identify in the HTML:

-   year

-   datePublished

-   volume

-   startPage

-   endPage

-   fileName

-   keywords

Once we do that, we'll need to further refine the table to match the expected input format for author names.

We also have some constants that can be entered for the following columns:

-   licenseURL

-   copyrightHolder

Let's go look at the HTML code for article 42_1 again and locate a couple of approaches to finding this information.

```{r test using meta tags}

```

```{r function pluck}

```

```{r test using map and pluck}

```

test extracting datePublished, volume, startPage, lastPage

```{r add to our tibble}

```

Unresolved problem:

Here is how the University of Alberta CSV file needs the names:

You can have multiple authors in the "authors" field by separating them with a semi-colon. Also, use a comma to separating first and last names. Example: Smith, John;Johnson, Jane ...

Variations:

1.  One author
2.  Multiple authors, with names separated by either "," or "and"

What I'd suggest:

Use separate_wider_regex() to split this column into separate authors.

Then split the author names on the last space as the delimiter.

Join the author names with the new delimiter.

Let's find out how another student approached this webscraping project in Python.

## Last thoughts

#### Learning objectives

At the end of the course, students are expected to be able to:

-   Demonstrate in depth understanding of the principles, motivations and goals for reproducible, ethical, and open data

    -   RDM lecture, Retraction Watch assignment, Open Science lecture by Dr. Vincent Lariviere

-   Use Git for communication and reproducible version control

    -   Assignment submissions

-   Import and tidy diverse data sources across platforms

    -   Statistics Canada data, research dataset from Borealis, webscraping TAC

-   Explore data to identify potential research questions or problems in the dataset

    -   *TAC* metadata inconsistencies

-   Identify best practices for research data management, including data organization, storage, security, sharing, and ethical re-use

    -   LEGO workshop (documentation), Research Data Management readings, Retraction Watch assignment, RDM examples

-   Demonstrate what they have learned about data acquisition, data organization, and data tools through self-reflection

    -   Ungrading conversations

## Last to-dos:

### CEWIL Survey

Dear [student]:

Please take the time to fill out this brief survey about your recently funded experience, TACking Towards the Future, which was funded in part through the CEWIL iHub grant and the Government of Canada’s Innovative Work-Integrated Learning Program.

***Your experience is valuable in helping us to improve the effectiveness of our programming***. The survey is very brief and will only take about 5 minutes to complete. Please click the link below to go to the survey web site (or copy and paste the link into your Internet browser).  You will be asked for the project number.

**Your project number is 2024-R2-E1770**

**Survey link:** [CEWIL iHub Student Exit Survey](https://forms.office.com/r/Z5ibuUDc8s)

Survey responses will be shared with the Government of Canada. All individual survey responses are anonymous, and no personally identifiable information will be associated with your responses to any reports of these data.

Thank you very much for your time and cooperation. Feedback from students is very important to CEWIL Canada and the Government of Canada.

2.  Project Number: **2024-R2-E1770**
3.  Community Partner: Theory & Application of Categories Editorial Board
4.  Industry / Community Supervisor's Name: Dr. Geoffrey Cruttwell
5.  Industry / Community Supervisor's Name: gcruttwell\@mta.ca

### Student Experience Survey (Mount Allison / available on Moodle)

### Ungrading Meetings next week (no class!)
